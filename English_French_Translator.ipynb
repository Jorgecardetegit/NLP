{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOYJNh+dWlCa2eW35tiYY3E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jorgecardetegit/NLP/blob/main/English_French_Translator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "EGZrDdkcP4_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import cv2"
      ],
      "metadata": {
        "id": "cPrTWyjwO0gx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.layers import (Dense,Flatten,SimpleRNN,InputLayer,Conv1D,Bidirectional,GRU,LSTM,BatchNormalization,Dropout,Input, Embedding,TextVectorization)\n",
        "from tensorflow.keras.losses import BinaryCrossentropy,CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import Accuracy,TopKCategoricalAccuracy, CategoricalAccuracy, SparseCategoricalAccuracy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "from tensorboard.plugins import projector"
      ],
      "metadata": {
        "id": "jQI1shtqPcnA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.metrics import confusion_matrix, roc_curve"
      ],
      "metadata": {
        "id": "jZZOe6Y_O0jD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "import datetime\n",
        "import pathlib"
      ],
      "metadata": {
        "id": "SvifO9OBPIun"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "vnxXSNCyPgJv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "3NPOO8bIlz2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manythings dataset"
      ],
      "metadata": {
        "id": "np59Wgl0SkFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.manythings.org/anki/fra-eng.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LQs0y09Q_vN",
        "outputId": "a2c1b380-8bee-41e4-95f1-93c14c40cc0c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-21 13:28:41--  https://www.manythings.org/anki/fra-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7757635 (7.4M) [application/zip]\n",
            "Saving to: ‘fra-eng.zip.1’\n",
            "\n",
            "fra-eng.zip.1       100%[===================>]   7.40M  11.4MB/s    in 0.6s    \n",
            "\n",
            "2023-10-21 13:28:42 (11.4 MB/s) - ‘fra-eng.zip.1’ saved [7757635/7757635]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/fra-eng.zip\" -d \"/content/dataset/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "untl0lYWQ_xV",
        "outputId": "205a7d26-787e-416d-d5c4-02905b43aa8b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/fra-eng.zip\n",
            "replace /content/dataset/_about.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_dataset=tf.data.TextLineDataset(\"/content/dataset/fra.txt\")"
      ],
      "metadata": {
        "id": "QZqIBxEQVgTe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Kaggle dataset"
      ],
      "metadata": {
        "id": "59TIenbdRWda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d dhruvildave/en-fr-translation-dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFptumgWQ_zl",
        "outputId": "d0341a6d-6606-4fff-868c-46b328924830"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/__init__.py\", line 23, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/api/kaggle_api_extended.py\", line 403, in authenticate\n",
            "    raise IOError('Could not find {}. Make sure it\\'s located in'\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/en-fr-translation-dataset.zip\" -d \"/content/dataset/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XTfMHgrQ_1j",
        "outputId": "56557438-6913-4529-bdec-d613dbcb0162"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/en-fr-translation-dataset.zip, /content/en-fr-translation-dataset.zip.zip or /content/en-fr-translation-dataset.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.experimental.CsvDataset(\"/content/dataset/en-fr.csv\",\n",
        " [tf.string,tf.string],) # List that specifies the data types of the columns in the CSV file\n"
      ],
      "metadata": {
        "id": "jOTQUJfhTsOp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic EDA"
      ],
      "metadata": {
        "id": "wzqJq3AsZGLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_dataset=tf.data.TextLineDataset(\"/content/dataset/fra.txt\")"
      ],
      "metadata": {
        "id": "91EC_PRmSpo5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tf.data.TextLineDataset reads lines from the text file lazily, which means it doesn't load the entire file into memory. As a result, there's no direct method on the TextLineDataset object to get the total number of lines (or length) immediately."
      ],
      "metadata": {
        "id": "NvsqOahtXJ2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_text_dataset = 0\n",
        "for _ in text_dataset:\n",
        "    count_text_dataset += 1"
      ],
      "metadata": {
        "id": "Cp5Gk3fYXJSr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The dataset has a total of {count_text_dataset} terms\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZaVlPyHS0Zb",
        "outputId": "70183ef7-bdc5-43c6-b4ba-042887b03ff5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dataset has a total of 227815 terms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets find the number of words in each element and get some descriptive statistics."
      ],
      "metadata": {
        "id": "04oecaqjYayo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_counts = [len(tf.strings.split(i, \" \")) for i in text_dataset]"
      ],
      "metadata": {
        "id": "eGjqppZgYZ4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "max_len = np.max(word_counts)\n",
        "mean_len = np.mean(word_counts)\n",
        "median_len = np.median(word_counts)\n",
        "percentile_95 = np.percentile(word_counts, 95)\n",
        "percentile_99 = np.percentile(word_counts, 99)"
      ],
      "metadata": {
        "id": "fVinickwYoJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.hist(word_counts, bins=50, edgecolor='k')\n",
        "plt.axvline(mean_len, color='r', linestyle='dashed', linewidth=1, label='Mean')\n",
        "plt.axvline(median_len, color='g', linestyle='dashed', linewidth=1, label='Median')\n",
        "plt.axvline(percentile_95, color='b', linestyle='dashed', linewidth=1, label='95th Percentile')\n",
        "plt.legend()\n",
        "plt.title(\"Distribution of Sequence Lengths\")\n",
        "plt.xlabel(\"Sequence Length\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FRpTUAg3YsQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data processing"
      ],
      "metadata": {
        "id": "TE7xlXLQSqMw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing class"
      ],
      "metadata": {
        "id": "EyBnlh1wZqdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 20000\n",
        "ENGLISH_SEQUENCE_LENGTH = 64\n",
        "FRENCH_SEQUENCE_LENGTH = 64\n",
        "EMBEDDING_DIM = 300\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "english_vectorize_layer = TextVectorization(\n",
        "  standardize='lower_and_strip_punctuation',                     # Convert to lowercase and remove punctuation\n",
        "  max_tokens= VOCAB_SIZE,                                        # Vocab contains 20000 terms and additional tokens will be treated as out-of-vocabulary (OOV)\n",
        "  output_mode='int',                                             # Tokens will be represented as an integer corresponding to the token's index in the vocabulary\n",
        "  output_sequence_length = ENGLISH_SEQUENCE_LENGTH)              # Sequence length of 64 (same as the french dataset sequence length)\n",
        "\n",
        "french_vectorize_layer = TextVectorization(                      # It´s important to include a layer for each category even it seems both layers are doing the same. Later on we will have to adapt each layer to its class\n",
        "  standardize='lower_and_strip_punctuation',\n",
        "  max_tokens= VOCAB_SIZE,\n",
        "  output_mode='int',\n",
        "  output_sequence_length = FRENCH_SEQUENCE_LENGTH)"
      ],
      "metadata": {
        "id": "WpUGzobFfINg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class preprocessing:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.VOCAB_SIZE = VOCAB_SIZE\n",
        "    self.FRENCH_SEQUENCE_LENGTH = FRENCH_SEQUENCE_LENGTH\n",
        "    self.ENGLISH_SEQUENCE_LENGTH = ENGLISH_SEQUENCE_LENGTH\n",
        "    self.EMBEDDING_DIM = EMBEDDING_DIM\n",
        "    self.BATCH_SIZE = BATCH_SIZE\n",
        "\n",
        "    self.english_vectorize_layer = english_vectorize_layer\n",
        "    self.french_vectorize_layer = french_vectorize_layer\n",
        "\n",
        "  def selector(self, input_text):\n",
        "    split_text=tf.strings.split(input_text,'\\t')\n",
        "    return {'input_1':split_text[0:1],'input_2':'starttoken '+split_text[1:2]},split_text[1:2]+' endtoken'\n",
        "\n",
        "  def separator(self, input_text):\n",
        "    split_text=tf.strings.split(input_text,'\\t')\n",
        "    return split_text[0:1],'starttoken '+split_text[1:2]+' endtoken'\n",
        "\n",
        "  def vectorizer(self, inputs, output):\n",
        "    return {'input_1':english_vectorize_layer(inputs['input_1']),\n",
        "            'input_2':french_vectorize_layer(inputs['input_2'])},french_vectorize_layer(output)\n",
        "\n",
        "prep_object = preprocessing()"
      ],
      "metadata": {
        "id": "AoVlhipXcMJK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Split dataset"
      ],
      "metadata": {
        "id": "imzxu-V_Y-eB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split_dataset = text_dataset.map(prep_object.selector)\n",
        "init_dataset = text_dataset.map(prep_object.separator)    # Separate french and english label."
      ],
      "metadata": {
        "id": "X3v6MZs4cS-g"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in split_dataset.take(1): print(i)\n",
        "print(\" \")\n",
        "for i in text_dataset.take(1): print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGfdEzxfXWj-",
        "outputId": "c939c83b-dfa9-4df2-cb72-eba8717124e6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "({'input_1': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, 'input_2': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttoken Va !'], dtype=object)>}, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Va ! endtoken'], dtype=object)>)\n",
            " \n",
            "tf.Tensor(b'Go.\\tVa !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Divide the dataset"
      ],
      "metadata": {
        "id": "nOrnR5DnbE8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "english_training_data=init_dataset.map(lambda x,y:x)\n",
        "french_training_data=init_dataset.map(lambda x,y:y)"
      ],
      "metadata": {
        "id": "9maBB3Z7ZOfy"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Adapt the vectorize layers"
      ],
      "metadata": {
        "id": "c2bpF6-kbl_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "english_vectorize_layer.adapt(english_training_data)\n",
        "french_vectorize_layer.adapt(french_training_data)"
      ],
      "metadata": {
        "id": "rOH50LIZcdPg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Vectorize the dataset"
      ],
      "metadata": {
        "id": "buS1u8nCftah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = split_dataset.map(prep_object.vectorizer)"
      ],
      "metadata": {
        "id": "pCXFB6EAcdRq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Shuffle and Prefetch"
      ],
      "metadata": {
        "id": "cmLQ6zrTsiSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=dataset.shuffle(2048).unbatch().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "NUM_BATCHES=int(200000/BATCH_SIZE)"
      ],
      "metadata": {
        "id": "rmTgTRtqsleV"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Split in training and validation"
      ],
      "metadata": {
        "id": "WHAqk9Kqs2LD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=dataset.take(int(0.9*NUM_BATCHES))\n",
        "val_dataset=dataset.skip(int(0.9*NUM_BATCHES))"
      ],
      "metadata": {
        "id": "Q-4KU4_LswZ6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model architecture"
      ],
      "metadata": {
        "id": "j5bjfxUNhs28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seq2Seq (GRU)"
      ],
      "metadata": {
        "id": "SdrRCYbmmB58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_UNITS = 256\n",
        "\n",
        "### Encoder\n",
        "input = Input(shape=(ENGLISH_SEQUENCE_LENGTH,), dtype=\"int64\", name=\"input_1\")\n",
        "x=Embedding(VOCAB_SIZE, EMBEDDING_DIM, )(input)\n",
        "encoded_input=Bidirectional(GRU(NUM_UNITS), )(x)\n",
        "\n",
        "### DECODER\n",
        "shifted_target=Input(shape=(FRENCH_SEQUENCE_LENGTH,), dtype=\"int64\", name=\"input_2\")\n",
        "x=Embedding(VOCAB_SIZE,EMBEDDING_DIM,)(shifted_target)\n",
        "x = GRU(NUM_UNITS*2, return_sequences=True)(x, initial_state=encoded_input)\n",
        "\n",
        "### OUTPUT\n",
        "x = Dropout(0.5)(x)\n",
        "target=Dense(VOCAB_SIZE,activation=\"softmax\")(x)\n",
        "seq2seq_gru=Model([input,shifted_target],target)"
      ],
      "metadata": {
        "id": "zY0E_kIFcdTw"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq_gru.summary()"
      ],
      "metadata": {
        "id": "MQ0kgy8HcdWE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6623e3c0-6e8f-43a8-d9f0-a32273e15cac"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 64)]                 0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 64)]                 0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 64, 300)              6000000   ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     (None, 64, 300)              6000000   ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " bidirectional (Bidirection  (None, 512)                  857088    ['embedding[0][0]']           \n",
            " al)                                                                                              \n",
            "                                                                                                  \n",
            " gru_1 (GRU)                 (None, 64, 512)              1250304   ['embedding_1[0][0]',         \n",
            "                                                                     'bidirectional[0][0]']       \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 64, 512)              0         ['gru_1[0][0]']               \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 64, 20000)            1026000   ['dropout[0][0]']             \n",
            "                                                          0                                       \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 24367392 (92.95 MB)\n",
            "Trainable params: 24367392 (92.95 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model training"
      ],
      "metadata": {
        "id": "yVb9LfpJwEJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bleu scoring class"
      ],
      "metadata": {
        "id": "QA6u_NyZZ3r3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BLEU(tf.keras.metrics.Metric):\n",
        "    def __init__(self,name='bleu_score'):\n",
        "        super(BLEU,self).__init__()\n",
        "        self.bleu_score=0\n",
        "\n",
        "    def update_state(self,y_true,y_pred,sample_weight=None):\n",
        "      y_pred=tf.argmax(y_pred,-1)\n",
        "      self.bleu_score=0\n",
        "      for i,j in zip(y_pred,y_true):\n",
        "        tf.autograph.experimental.set_loop_options()\n",
        "\n",
        "        total_words=tf.math.count_nonzero(i)\n",
        "        total_matches=0\n",
        "        for word in i:\n",
        "          if word==0:\n",
        "            break\n",
        "          for q in range(len(j)):\n",
        "            if j[q]==0:\n",
        "              break\n",
        "            if word==j[q]:\n",
        "              total_matches+=1\n",
        "              j=tf.boolean_mask(j,[False if y==q else True for y in range(len(j))])\n",
        "              break\n",
        "\n",
        "        self.bleu_score+=total_matches/total_words\n",
        "\n",
        "    def result(self):\n",
        "        return self.bleu_score/BATCH_SIZE"
      ],
      "metadata": {
        "id": "PD3V_iwsZCMH"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq_gru.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),     # Used because outputs are not one hot representations\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "    metrics=[BLEU(), \"accuracy\"],\n",
        "    run_eagerly=True)                                         # Method has to be set to eager excecution."
      ],
      "metadata": {
        "id": "60hKS0JxcdYC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=seq2seq_gru.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=5)"
      ],
      "metadata": {
        "id": "yVJTibZzcdaF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e167dc1-f727-4589-cfac-bbfe4830ddd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7fc225de3eb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7fc225de3eb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method BLEU.update_state of <__main__.BLEU object at 0x7fc191e1eb90>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: not enough values to unpack (expected 2, got 0)\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <bound method BLEU.update_state of <__main__.BLEU object at 0x7fc191e1eb90>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: not enough values to unpack (expected 2, got 0)\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "    588/Unknown - 986s 2s/step - loss: 1.2424 - bleu: 0.2365 - accuracy: 0.9221"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model evaluation"
      ],
      "metadata": {
        "id": "RKzjDH1Cw1dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model_loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "datewU8zw4Q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "\n",
        "plt.title('model_accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Wmp1A9Wvw8Kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "MMLkHa-WxqJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_word={x:y for x, y in zip(range(len(french_vectorize_layer.get_vocabulary())),\n",
        "                                   french_vectorize_layer.get_vocabulary())}"
      ],
      "metadata": {
        "id": "wg_D5ts3w8NJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translator(english_sentence):\n",
        "  tokenized_english_sentence=english_vectorize_layer([english_sentence])\n",
        "  shifted_target='starttoken'\n",
        "\n",
        "  for i in range(FRENCH_SEQUENCE_LENGTH):\n",
        "    tokenized_shifted_target=french_vectorize_layer([shifted_target])\n",
        "    output=seq2seq_gru.predict([tokenized_english_sentence,tokenized_shifted_target])\n",
        "    french_word_index=tf.argmax(output,axis=-1)[0][i].numpy()\n",
        "    current_word=index_to_word[french_word_index]\n",
        "    if current_word=='endtoken':\n",
        "      break\n",
        "    shifted_target+=' '+current_word\n",
        "  return shifted_target[11:]"
      ],
      "metadata": {
        "id": "s5QLvpwzw8Rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PwPDfb_zw8VP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}